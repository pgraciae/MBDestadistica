---
title: "Practica1"
author: "Sebastian Cueva, Pol Gracia"
date: "20/11/2022"
output:
  html_notebook: default
  html_document: default
---

# 0. Imports

#### Importar librerias

```{r}
library(tidyverse)
library(tidyr)
library(dplyr)
library(knitr)
library(ggplot2)
library(stringr)
library(reshape)
library(minerva)
library(heatmaply)
library(kableExtra)
library(factoextra)
library(ggbiplot)
library(randomForest)
library(ROCR)
```

#### Importar datos

Aunque se nos entrega el fichero de train entero y podriamos obtener la variable respuesta, se trataran cómo si de un caso real se tratara, y todo el EDA y el training se usara solo el p1_train. El p1_test se usara únicamente para validar los resultados al final de todo de la practica.

```{r}
train <- read.csv("./data/train.csv", head = TRUE, stringsAsFactors=TRUE)

p1_train <- read.csv("./data/p1_train.csv", head = TRUE, sep=";", stringsAsFactors=TRUE)

p1_test <- read.csv("./data/p1_test.csv", sep=";", head = TRUE, stringsAsFactors=TRUE)
```



# 1. Exploratory Data Analysis

En este apartado se va hacer el análisi exploratorio de los datos.

```{r}
head(p1_train)
```
Eliminamos la variable id del dataset ya que es el indice, y no aporta información.

```{r}
p1_train <- p1_train %>% select(-id)
```


### Para los datos : 

#### 000. Nans handling

Se puede apreciar que no encontramos ningún nan en las columnas del dataset, haciendo que no tengamos que tratar con ellos.

```{r}
p1_train %>% group_by() %>% summarise_all(funs(sum(is.na(.))))
```


#### 00. Duplicates handling

Se eliminan las filas duplicadas del dataset.

```{r}
p1_train <- p1_train %>%
                distinct()
p1_train
```
## Separar entre target y datos y hacer:

Eliminamos la variable target del dataset

```{r}
predictors <- p1_train %>% select(-count)
```


#### 0. Separar datos entre numericos y categoricos

Para las variables que son categoricas, aunque su representación sea numerica debemos tratarlas como categoricas en el dato para obtener el resultado deseado.
Las variables categoricas del dataset son: workingday, holiday, weather y season.

```{r}
predictors <- predictors %>% mutate(workingday = as.factor(workingday),
                                   holiday = as.factor(holiday),
                                   weather = as.factor(weather),
                                   season = as.factor(season))
```


#### 1. Hacer boxplots de los campos y mirar outliers en los datos

Eliminamos variables categoricas

```{r}
meltPred <- predictors %>% select(where(is.numeric)) %>% melt()
meltPred %>%
 ggplot(aes(factor(variable), value)) +
   geom_violin(width=1, color = "gray", alpha = 0.2, fill = 'green' ) +
    geom_boxplot(width=0.3, color="black", fill='blue', alpha=0.3, outlier.colour="red", outlier.shape=8,
             outlier.size=1, notch=TRUE) + facet_wrap(~variable, scale="free")
```


#### 2. matriz de correlaciones con la variable target

```{r}
numpred <- predictors %>% select(where(is.numeric)) 
corrs <- data.frame(MIC = mine(numpred, y = p1_train$count, alpha = 0.7)) %>% select(MIC.Y)
corrs$Pearson <- cor(numpred, y = p1_train$count, method = 'pearson')
rownames(corrs) <- rownames(corrs$Pearson)
corrs
```

```{r}
cbind(corrs$Pearson, corrs$Pearson ) %>% heatmaply_cor(xlab = "",
              ylab = "Columnas", main = "Correlacion Pearson de las columnas numericas con la variable target 'count'", cellnote = cbind(corrs$Pearson, corrs$Pearson ), k_col = 1, k_row = 1)
```


```{r}
cbind(corrs$MIC.Y, corrs$MIC.Y ) %>% heatmaply_cor(xlab = "",
              ylab = "Columnas", main = "Correlacion MIC de las columnas numericas con la variable target 'count'", cellnote = cbind(corrs$MIC.Y, corrs$MIC.Y ), k_col = 1, k_row = 1)

```

#### 3. PCA 


```{r}
pca <- predictors %>% select(where(is.numeric)) %>% sample_n(200) %>%
  prcomp(scale. = TRUE, center = TRUE)
summary(pca)
```

```{r}
pca %>% 
  ggbiplot::ggbiplot(scale = 1)
```

### Para el target : 

#### 0. Analisis lógico: variable categorica o continua, max mins, boxplots...


La variable target 'count' es una variable categorica que representa el numero de bicicletas alquiladas en las grandes ciudades. La variable tiene una media de 190 y un std de 182, un valor bastante elevado que indica mucha variación en la variable.

```{r}
corrs$Pearson
```

```{r}
mean(p1_train$count)
sd(p1_train$count)
```
El grafico siguiente es un boxplot y un violin plot sobre la variable target 'count'. Por el violin plot podemos ver que la mayoria de valores se encuentran cercanos al 0 pero por el boxplot vemos que los valores altos del dataset hacen que la media suba considerablemente. En el boxplot podemos apreciar también diversos outliers que sobresalen a partir del valor 550.

```{r}
p1_train %>%
  ggplot( aes(x=count, y=count)) +
    geom_violin(width=1, color = "gray", alpha = 0.2, fill = 'green' ) +
    geom_boxplot(width=0.3, color="black", fill='blue', alpha=0.3, outlier.colour="red", outlier.shape=8,
             outlier.size=1, notch=TRUE) +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    ggtitle("Violin y box plot de los valores de la variable target ") +
    xlab("") + theme_classic() 

```


#### 1. Hacer barplot(o otro) para mirar la distribución de los targets (son balanceados)

Partiendo de la base que la variable target es continua, podemos apreciar que la frequencia de la misma sigue una distribución parecida a la exponencial, ya que cómo más se aleja del número 0 menos frequencia encontramos.
Habrá que tener en cuenta entonces, que el modelo va a tender a predecir valores bajos en la mayoria de los casos.

```{r}
p1_train %>%
ggplot( aes(x=count)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth = 20)+
 geom_density(alpha=.2, fill="#FF6666") + theme_classic()

```



#### 2. Encontrar relaciones lògicas con las variables de datos y hacer plots (alquier por dias que llueve, alquiler por estaciones, alquiler por festivo o laborable...) - hacer 4 o 5 plots de relaciones para tener contento a Jordi :) (mirar ggplot2) - libreria plots

Suma de bicicletas alquiladas cuando es working day y cuando no

Suma de bicicletas alquiladas cuando es holiday

Suma de bicicletas alquiladas por season

Suma de bicicletas alquiladas por hora



## 2. MODELO

#### 1. Normalización y preparación de los datos

```{r}
p1_train %>% mutate(across(where(is.numeric), scale))
```

#### 2. Seleccion modelo

# linear model
```{r}
mod_lm <- lm(count~., p1_train) 
mean(mod_lm$residuals^2) #mse
```
se puede apreciar que el mse es muy grande
```{r}
plot(p1_train$count)
abline(mod_lm)
```


# glm
```{r}
mod_glm <- glm(count~., p1_train, family = poisson())
mean(mod_glm$residuals^2)
```
No tiene sentido

# Random forest
```{r}
set.seed(1234)
rf <- randomForest(count~., data = p1_train, mtry = 10, importance = TRUE, ntree = 50 )
rf
```

```{r}
importance(rf)
varImpPlot(rf)
```
Model performance (mse)
```{r}
mseDF <- data.frame(pred = rf$predicted, gt = rf$y)
mean((mseDF$pred-mseDF$gt)^2) #mse
```
mse bastante correcto

```{r}
mseDF
```
Volvemos a hacer random forest con variables más importantes

```{r}
p1_trainImp <- p1_train %>% select('hour','year','workingday','season','humidity', 'count')
p1_trainImp
```
```{r}
rf2 <- randomForest(count~., data = p1_trainImp, mtry = 6, importance = TRUE, ntree = 50 )
rf2
```
Fucniona peor cogiendo solo las variables más importantes (mse 4142 > mse 2193)

## 3. Conclusiones



